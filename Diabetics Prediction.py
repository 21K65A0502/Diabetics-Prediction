# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZM5R78llJiDcg_L93OgC549_iPOgdPlV
"""

import pandas as pd
import numpy as np

df=pd.read_csv("/content/Healthcare-Diabetes.csv")

df.head()

#1.DataTypes
df.dtypes

#2.Statistical Functions
df.describe()

#3.Data Preprocessing and Attribute Selection
#3.1: Handling Values
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

df.isnull().sum()

# Handle missing values (if any)
df = df.dropna()
df

# Define features and target variable
X = df.drop(columns=["BloodPressure"])
y = df["BloodPressure"]

# Normalize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Print dataset shape
print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

# Function to remove outliers using IQR
def remove_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1

    # Define the valid range
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Remove outliers
    return data[~((data < lower_bound) | (data > upper_bound)).any(axis=1)]

# Remove outliers
df_cleaned = remove_outliers(df)

# Display shape before and after removing outliers
print(f"Original dataset shape: {df.shape}")
print(f"Dataset shape after removing outliers: {df_cleaned.shape}")

#4 Attribute Selection
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.linear_model import LogisticRegression

# Separate features and target
X = df.drop(columns=['BloodPressure'])  # Features
y = df['BloodPressure']  # Target variable

# Normalize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Method 1: Feature Importance using RandomForest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_scaled, y)
feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
print("Feature Importance:\n", feature_importances)

# Method 2: SelectKBest using Chi-Square test
from sklearn.preprocessing import MinMaxScaler  # Import MinMaxScaler

# Normalize numerical features using MinMaxScaler
scaler = MinMaxScaler()  # Replace StandardScaler with MinMaxScaler
X_scaled = scaler.fit_transform(X)

chi2_selector = SelectKBest(score_func=chi2, k=5)  # Select top 5 features
X_kbest = chi2_selector.fit_transform(X_scaled, y)  # Now use the MinMax scaled data
chi2_scores = pd.Series(chi2_selector.scores_, index=X.columns).sort_values(ascending=False)
print("\nChi-Square Scores:\n", chi2_scores)
print("Selected Features (Chi-Square):\n", X.columns[chi2_selector.get_support()])

#4. Graphical Representation of the data
import matplotlib.pyplot as plt
import seaborn as sns

# Set plot style
sns.set_style("whitegrid")

# 1. Histogram of Numerical Features
df.hist(figsize=(12, 8), bins=20, edgecolor='black')
plt.suptitle("Distribution of Numerical Features", fontsize=16)
plt.show()

# 2. Boxplots for Outlier Detection
plt.figure(figsize=(12, 6))
sns.boxplot(data=df)
plt.title("Boxplot of Features")
plt.xticks(rotation=45)
plt.show()

# 3. Correlation Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

# 4. Churn Countplot
plt.figure(figsize=(6, 4))
sns.countplot(x="Churn", data=df, palette="pastel")
plt.title("Churn Count Distribution")
plt.xlabel("Churn (0 = No, 1 = Yes)")
plt.ylabel("Count")
plt.show()

# 5. Pairplot of Important Features
sns.pairplot(df, hue="Churn", diag_kind="kde", corner=True)
plt.show()

#6. Data Cleaning and Transformation
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler


### ðŸ”¹ Step 1: Handle Missing Values
# Check for missing values
print("Missing values before handling:\n", df.isnull().sum())

# Fill missing values with median for numerical features
df.fillna(df.median(numeric_only=True), inplace=True)

# Drop rows with any remaining missing values (if categorical)
df.dropna(inplace=True)

print("\nMissing values after handling:\n", df.isnull().sum())

### ðŸ”¹ Step 2: Remove Duplicates
df.drop_duplicates(inplace=True)

### ðŸ”¹ Step 3: Convert Data Types (if needed)
# Example: If any numeric column is stored as an object (string), convert it
for col in df.columns:
    if df[col].dtype == 'object':
        try:
            df[col] = pd.to_numeric(df[col])
        except:
            pass  # Skip if conversion is not possible

### ðŸ”¹ Step 4: Remove Outliers (IQR Method - Already Covered)
def remove_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[~((data < lower_bound) | (data > upper_bound)).any(axis=1)]

df = remove_outliers(df)

### ðŸ”¹ Step 5: Encode Categorical Variables (If Any)
# Convert binary categorical features into 0 and 1
categorical_columns = ['ContractRenewal', 'DataPlan']
df[categorical_columns] = df[categorical_columns].astype(int)  # Ensure they are integer

### ðŸ”¹ Step 6: Feature Scaling
scaler = StandardScaler()
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns  # Select numerical features
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

### ðŸ”¹ Final Data Check
print("\nCleaned Dataset:\n", df.head())
print("\nDataset Shape After Cleaning:", df.shape)

#7. Linear Regression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


### ðŸ”¹ Step 1: Select Features and Target
X = df.drop(columns=['MonthlyCharge'])  # Independent Variables
y = df['MonthlyCharge']  # Dependent Variable

### ðŸ”¹ Step 2: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

### ðŸ”¹ Step 3: Train Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)

### ðŸ”¹ Step 4: Predictions
y_pred = model.predict(X_test)

### ðŸ”¹ Step 5: Model Evaluation
r2 = r2_score(y_test, y_pred)  # RÂ² Score
rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # RMSE

print(f"RÂ² Score: {r2:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")

### ðŸ”¹ Step 6: Visualization (Actual vs. Predicted)
plt.figure(figsize=(8, 5))
plt.scatter(y_test, y_pred, color="blue", alpha=0.6)
plt.plot(y_test, y_test, color="red", linestyle="dashed", label="Perfect Fit")
plt.xlabel("Actual Monthly Charge")
plt.ylabel("Predicted Monthly Charge")
plt.title("Actual vs Predicted Values")
plt.legend()
plt.show()

#8.Feature Selection with INfo Gain
import pandas as pd
import numpy as np
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import LabelEncoder


### ðŸ”¹ Step 1: Prepare Data
X = df.drop(columns=['Churn'])  # Independent Variables
y = df['Churn']  # Target Variable

# Convert categorical columns (if any) using Label Encoding
categorical_columns = ['ContractRenewal', 'DataPlan']
for col in categorical_columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])

### ðŸ”¹ Step 2: Compute Mutual Information (Info Gain)
mi_scores = mutual_info_classif(X, y)  # Compute MI Scores
mi_series = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)

# Display ranked features based on MI
print("\nFeature Importance based on Information Gain:\n")
print(mi_series)

### ðŸ”¹ Step 3: Select Top K Features (Example: Top 5)
top_k = 5
selected_features = mi_series.nlargest(top_k).index
print("\nTop 5 Selected Features:\n", list(selected_features))

#9. Classification
#9. Classification
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

### ðŸ”¹ Step 1: Data Preprocessing
X = df.drop(columns=['Churn'])  # Features
y = df['Churn']  # Target Variable

# Convert categorical columns to numeric
categorical_columns = ['ContractRenewal', 'DataPlan']
for col in categorical_columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])

# Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

### ðŸ”¹ Step 2: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)
# Added stratify=y to ensure both classes are present in train and test sets

### ðŸ”¹ Step 3: Train Classification Models

# Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
# ... (rest of the code remains the same)

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Support Vector Machine (SVM)
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

### ðŸ”¹ Step 4: Model Evaluation

models = {'Logistic Regression': log_reg, 'Random Forest': rf, 'SVM': svm, 'KNN': knn}

for name, model in models.items():
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n{name} Accuracy: {accuracy:.4f}")
    print(classification_report(y_test, y_pred))

### ðŸ”¹ Step 5: Confusion Matrix for Best Model (Random Forest)
best_model = rf  # Choose the best-performing model
y_pred_best = best_model.predict(X_test)

plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_best), annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Random Forest")
plt.show()